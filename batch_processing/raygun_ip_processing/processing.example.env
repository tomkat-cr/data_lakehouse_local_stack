# Root directory path
BASE_PATH=/home/LocalLakeHouse/Project

# Debug flag
DEBUG=0

# To process all JSON files:
# TESTING_ITERACTIONS=
# To process a small set of JSON files:
TESTING_ITERACTIONS=1

# S3 pagination page size: 1000 files chunks
S3_PAGE_SIZE=1000

# S3 read timeout: 120 seconds
S3_READ_TIMEOUT=120

# Local directory path containing JSON files
INPUT_LOCAL_DIRECTORY="data/raygun"

# S3 prefix (directory path in the bucket) to store raw data read from
# the local directory
S3_PREFIX="Raw/raygun"

# Desired attribute and alias to filter one column
DESIRED_ATTRIBUTE="Request.IpAddress"
DESIRED_ALIAS=RequestIpAddress

# Final output result file
RESULTS_SUB_DIRECTORY=raygun_ip_addresses_summary

# Number of batches to Save Data into Apache Hive
HIVE_BATCHES=10

# Hive metastore URIs
HIVE_METASTORE_URI=thrift://metastore:9083"

# Spark App name
SPARK_APPNAME=RaygunErrorTraceAnalysis

# Spark driver memory
# "3g" for small files it's better 2-3g
# "12g" for big files with more data it's better 4-5g
SPARK_DRIVER_MEMORY=3g

# Repartition the DataFrame to optimize parallel processing and memory usage
# (Adjust the number of partitions based on your environment and data size,
#  workload and cluster setup)
DF_NUM_PARTITIONS=200

# Define the batch size to read into dataframe the JSON files in batches
# (Adjust the batch size based on your memory capacity and data size)
DF_READ_BATCH_SIZE=5000

# Dataframe cluster storage bucket name
DF_CLUSTER_STORAGE_BUCKET_PREFIX="ClusterData/RaygunIpSummary"

# Dataframe output format
DF_OUTPUT_FORMAT=parquet

# Dataframe compression format
DF_COMPRESSION_FORMAT=snappy

# Include header in the Dataframe
DF_INPUT_HEADER=1
