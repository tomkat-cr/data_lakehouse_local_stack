# Root directory path
BASE_PATH="/home/PyCon2024/Project"

# Debug flag
DEBUG=0

# To process all JSON files:
# TESTING_ITERACTIONS=
# To process a small set of JSON files:
TESTING_ITERACTIONS=1

# S3 pagination page size: 1000 files chunks
S3_PAGE_SIZE=1000

# S3 read timeout: 120 seconds
S3_READ_TIMEOUT=120

# Local directory path containing JSON files
INPUT_LOCAL_DIRECTORY="data/raygun"

# S3 prefix (directory path in the bucket) to store raw data read from
# the local directory
S3_PREFIX="Raw/raygun"

# Desired attribute and alias to filter one column
DESIRED_ATTRIBUTE="Request.IpAddress"
DESIRED_ALIAS=RequestIpAddress

# Repartition the DataFrame to optimize parallel processing and memory usage
# (Adjust the number of partitions based on your environment and data size,
#  workload and cluster setup)
DF_NUM_PARTITIONS=200

# Define the batch size to read into dataframe the JSON files in batches
# (Adjust the batch size based on your memory capacity and data size)
DF_READ_BATCH_SIZE=5000

# Number of batches to Save Data into Apache Hive
HIVE_BATCHES=10

# Final output result file
RESULTS_SUB_DIRECTORY=raygun_ip_addresses_summary
